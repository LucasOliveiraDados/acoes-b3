# ACOES_BRASILEIRAS

Este projeto realiza a extraÃ§Ã£o, transformaÃ§Ã£o, anÃ¡lise e armazenamento de dados de aÃ§Ãµes da B3, com foco nas mais lÃ­quidas e nas variaÃ§Ãµes recentes de desempenho. Todo o pipeline foi desenvolvido em Python, orquestrado com **Apache Airflow**, containerizado com **Docker** e os dados finais analisados no **Power BI**.

---

## ğŸ“ Estrutura do Projeto

```
ACOES_BRASILEIRAS/
â”œâ”€â”€ codigos/
â”‚   â”œâ”€â”€ extrair_tickers.py         # Seleciona aÃ§Ãµes com liquidez â‰¥ R$ 5 milhÃµes (API brapi.dev)
â”‚   â”œâ”€â”€ acoes_dados.py             # Coleta histÃ³rico de preÃ§os via yfinance
â”‚   â”œâ”€â”€ valorizacao_acoes.py       # Calcula valorizaÃ§Ã£o diÃ¡ria e acumulada
â”‚   â”œâ”€â”€ salvar_dados_acoes.py      # Armazena no PostgreSQL e exporta em CSV
â”‚   â””â”€â”€ conexao_banco.py           # Gerencia conexÃ£o com o banco (via .env)
â”œâ”€â”€ main.py                        # Pipeline completo: extraÃ§Ã£o + transformaÃ§Ã£o + carga
â”œâ”€â”€ tickers_validos.txt            # Lista dos tickers filtrados
â”œâ”€â”€ .env                           # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ requirements.txt               # Bibliotecas do projeto
â”œâ”€â”€ airflow/                       # Estrutura do Airflow (Docker + dags)
```

---

## ğŸ”„ ETL â€“ Pipeline de Dados

A lÃ³gica de dados foi estruturada em trÃªs etapas principais:

### ğŸŸ¢ ExtraÃ§Ã£o

Coleta automÃ¡tica de dados de aÃ§Ãµes com base em critÃ©rios mÃ­nimos de liquidez.

- `extrair_tickers.py`: Consulta a API pÃºblica brapi.dev e filtra ativos com volume mÃ©dio diÃ¡rio â‰¥ R$ 5 milhÃµes.
- `acoes_dados.py`: Consulta o histÃ³rico de preÃ§os e volumes diÃ¡rios via yfinance.

### ğŸŸ¡ TransformaÃ§Ã£o

CÃ¡lculo de mÃ©tricas financeiras Ãºteis Ã  tomada de decisÃ£o.

- `valorizacao_acoes.py`: Calcula a valorizaÃ§Ã£o diÃ¡ria e valorizaÃ§Ã£o acumulada ao longo do perÃ­odo analisado, mantendo os valores com 2 casas decimais.

### ğŸŸ  Carga

PersistÃªncia dos dados transformados:

- Armazenamento em banco PostgreSQL (tabela `dados_acoes`)
- ExportaÃ§Ã£o em CSV local (`dados_acoes.csv`)

---

## âš™ï¸ OrquestraÃ§Ã£o com Airflow

A automaÃ§Ã£o das etapas foi feita com o Apache Airflow, permitindo agendamentos periÃ³dicos do pipeline.

- Estrutura completa com `docker-compose` e pastas `dags/`, `plugins/`, `logs/`
- DAG personalizada para executar o pipeline completo em ordem (extraÃ§Ã£o â†’ transformaÃ§Ã£o â†’ carga)
- Ambiente isolado com Docker, pronto para escalabilidade futura

---

## ğŸ³ Docker

Todo o ecossistema de execuÃ§Ã£o foi encapsulado em containers Docker:

- Banco de dados PostgreSQL
- Webserver e Scheduler do Airflow
- ExecuÃ§Ã£o do pipeline via `main.py` e/ou agendamento

---

## ğŸ“Š AnÃ¡lise Visual com Power BI

A anÃ¡lise dos dados transformados foi feita com Power BI Desktop, utilizando o arquivo `dados_acoes.csv` como fonte principal.

Principais KPIs e visualizaÃ§Ãµes:

- **MÃ©dia da valorizaÃ§Ã£o acumulada** (comparada Ã  meta da Selic)
- **ProporÃ§Ã£o de aÃ§Ãµes com desempenho positivo** (meta: â‰¥50%)
- **MÃ©dia de volume diÃ¡rio negociado**
- **GrÃ¡fico comparativo entre aÃ§Ãµes selecionadas**
- **Ranking de aÃ§Ãµes com maior valorizaÃ§Ã£o acumulada**
- **EvoluÃ§Ã£o de volume e preÃ§o por mÃªs**

---

## ğŸ”— Links

- CÃ³digo-fonte completo: [GitHub â€“ LucasOliveiraDados](https://github.com/LucasOliveiraDados)
- Dashboard interativo no Power BI Service: _(inserir link ao publicar)_

---

## âœï¸ Autor

**Lucas Oliveira**  
[GitHub](https://github.com/LucasOliveiraDados) Â· [LinkedIn](https://www.linkedin.com/)
